# data_arrow.py (ä¿®æ”¹ç‰ˆæœ¬)
import json
import os
import shutil
import argparse
import glob
from datasets import Dataset, concatenate_datasets
from tqdm import tqdm

def get_directory_size(path):
    """è®¡ç®—ç›®å½•å¤§å°"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            try:
                total_size += os.path.getsize(filepath)
            except (OSError, IOError):
                pass
    return total_size

def load_data_from_path(input_path):
    """
    ä»è·¯å¾„åŠ è½½æ•°æ®ï¼Œæ”¯æŒæ–‡ä»¶å’Œç›®å½•
    """
    all_data = []
    
    if os.path.isfile(input_path):
        # å•ä¸ªæ–‡ä»¶
        print(f"å¤„ç†å•ä¸ªæ–‡ä»¶: {input_path}")
        data = load_single_file(input_path)
        all_data.extend(data)
    
    elif os.path.isdir(input_path):
        # ç›®å½• - æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        print(f"å¤„ç†ç›®å½•: {input_path}")
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        patterns = ['*.json', '*.jsonl']
        
        files_found = []
        for pattern in patterns:
            files = glob.glob(os.path.join(input_path, pattern))
            files_found.extend(files)
        
        # é€’å½’æŸ¥æ‰¾å­ç›®å½•
        for pattern in patterns:
            files = glob.glob(os.path.join(input_path, '**', pattern), recursive=True)
            files_found.extend(files)
        
        # å»é‡å¹¶æ’åº
        files_found = sorted(list(set(files_found)))
        
        if not files_found:
            print(f"é”™è¯¯ï¼šåœ¨ç›®å½• {input_path} ä¸­æœªæ‰¾åˆ° .json æˆ– .jsonl æ–‡ä»¶")
            return []
        
        print(f"æ‰¾åˆ° {len(files_found)} ä¸ªæ•°æ®æ–‡ä»¶:")
        for file in files_found[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            file_size = os.path.getsize(file) / 1024 / 1024  # MB
            print(f"  - {os.path.basename(file)} ({file_size:.2f} MB)")
        if len(files_found) > 10:
            print(f"  ... è¿˜æœ‰ {len(files_found) - 10} ä¸ªæ–‡ä»¶")
        
        # é€ä¸ªå¤„ç†æ–‡ä»¶
        for file_path in tqdm(files_found, desc="å¤„ç†æ–‡ä»¶"):
            try:
                data = load_single_file(file_path)
                all_data.extend(data)
                print(f"  âœ… {os.path.basename(file_path)}: {len(data)} æ¡æ•°æ®")
            except Exception as e:
                print(f"  âŒ {os.path.basename(file_path)}: å¤„ç†å¤±è´¥ - {e}")
                continue
    
    else:
        print(f"é”™è¯¯ï¼šè·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ³•è¯†åˆ«: {input_path}")
        return []
    
    return all_data

def load_single_file(file_path):
    """åŠ è½½å•ä¸ªæ–‡ä»¶"""
    data = []
    
    try:
        if file_path.endswith('.jsonl'):
            # JSONLæ ¼å¼
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f):
                    try:
                        item = json.loads(line.strip())
                        if item:  # è·³è¿‡ç©ºè¡Œ
                            data.append(item)
                    except json.JSONDecodeError as e:
                        if line_num < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                            print(f"    è·³è¿‡ç¬¬{line_num+1}è¡Œï¼ŒJSONè§£æé”™è¯¯: {e}")
                        continue
        
        elif file_path.endswith('.json'):
            # JSONæ ¼å¼
            with open(file_path, 'r', encoding='utf-8') as f:
                content = json.load(f)
                if isinstance(content, list):
                    data = content
                elif isinstance(content, dict):
                    data = [content]
                else:
                    print(f"    è­¦å‘Šï¼šæœªçŸ¥çš„JSONæ ¼å¼: {type(content)}")
        
        else:
            print(f"    è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
    
    except Exception as e:
        print(f"    è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    return data

def preprocess_gemini_dataset(input_path, output_path):
    """
    é¢„å¤„ç†æ•°æ®é›†
    
    Args:
        input_path: è¾“å…¥æ•°æ®é›†è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
        output_path: è¾“å‡ºæ•°æ®é›†è·¯å¾„
    """
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_path):
        print(f"é”™è¯¯ï¼šè¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return False
    
    print(f"è¾“å…¥æ•°æ®é›†: {input_path}")
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")
    
    # å¦‚æœè¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼Œåˆ é™¤
    if os.path.exists(output_path):
        print(f"åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•: {output_path}")
        shutil.rmtree(output_path)
    
    print("å¼€å§‹é¢„å¤„ç†æ•°æ®é›†...")
    
    try:
        # åŠ è½½æ•°æ®
        data = load_data_from_path(input_path)
        
        if not data:
            print("é”™è¯¯ï¼šæ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼")
            return False
        
        print(f"åŸå§‹æ•°æ®é‡: {len(data)}")
        
        # éªŒè¯å’Œæ¸…ç†æ•°æ®
        valid_data = []
        invalid_count = 0
        
        # æ£€æµ‹æ•°æ®æ ¼å¼
        if len(data) > 0:
            sample_item = data[0]
            print(f"æ•°æ®æ ·ä¾‹å­—æ®µ: {list(sample_item.keys()) if isinstance(sample_item, dict) else 'unknown'}")
        
        for item in tqdm(data, desc="éªŒè¯æ•°æ®"):
            if isinstance(item, dict):
                question_fields = ['question', 'input', 'prompt', 'instruction', 'query']
                response_fields = ['response', 'output', 'answer', 'completion', 'target']
                
                question = None
                response = None
                
                for field in question_fields:
                    if field in item and item[field] and str(item[field]).strip():
                        question = str(item[field]).strip()
                        break
                
                for field in response_fields:
                    if field in item and item[field] and str(item[field]).strip():
                        response = str(item[field]).strip()
                        break
                
                if question and response:
                    # æ ‡å‡†åŒ–å­—æ®µå
                    standardized_item = {
                        'question': question,
                        'response': response
                    }
                    # ä¿ç•™å…¶ä»–æœ‰ç”¨å­—æ®µ
                    for key in ['datasource', 'id', 'metadata', 'source']:
                        if key in item:
                            standardized_item[key] = item[key]
                    
                    valid_data.append(standardized_item)
                else:
                    invalid_count += 1
                    if invalid_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ— æ•ˆæ ·ä¾‹
                        print(f"æ— æ•ˆæ•°æ®æ ·ä¾‹ {invalid_count}: {list(item.keys()) if isinstance(item, dict) else item}")
            else:
                invalid_count += 1
        
        print(f"æœ‰æ•ˆæ•°æ®é‡: {len(valid_data)}")
        print(f"æ— æ•ˆæ•°æ®é‡: {invalid_count}")
        
        if len(valid_data) == 0:
            print("é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼")
            print("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼ã€‚æ•°æ®åº”è¯¥åŒ…å«ä»¥ä¸‹å­—æ®µç»„åˆï¼š")
            print("  é—®é¢˜å­—æ®µ: question, input, prompt, instruction, query")
            print("  å›ç­”å­—æ®µ: response, output, answer, completion, target")
            return False
        
        # æ˜¾ç¤ºç¤ºä¾‹æ•°æ®
        print("\nç¤ºä¾‹æ•°æ®:")
        example = json.dumps(valid_data[0], ensure_ascii=False, indent=2)
        print(example[:300] + ("..." if len(example) > 300 else ""))
        
        # è½¬æ¢ä¸ºDatasetå¹¶ä¿å­˜ä¸ºArrowæ ¼å¼
        print("è½¬æ¢ä¸ºDatasetæ ¼å¼...")
        dataset = Dataset.from_list(valid_data)
        
        print("ä¿å­˜åˆ°ç£ç›˜...")
        dataset.save_to_disk(output_path)
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        dir_size = get_directory_size(output_path)
        dir_size_mb = dir_size / 1024 / 1024
        
        print(f"\nâœ… é¢„å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ æ•°æ®ä¿å­˜åˆ°: {output_path}")
        print(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {dir_size_mb:.2f} MB")
        print(f"ğŸ“Š æ•°æ®æ¡æ•°: {len(dataset)}")
        
        # éªŒè¯ä¿å­˜çš„æ•°æ®
        print("ğŸ” éªŒè¯ä¿å­˜çš„æ•°æ®...")
        try:
            from datasets import load_from_disk
            loaded_dataset = load_from_disk(output_path)
            print(f"âœ… éªŒè¯æˆåŠŸï¼åŠ è½½äº† {len(loaded_dataset)} æ¡æ•°æ®")
            print("ğŸ“ ç¤ºä¾‹æ•°æ®:", loaded_dataset[0])
        except Exception as e:
            print(f"âŒ éªŒè¯å¤±è´¥: {e}")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ é¢„å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(description='é¢„å¤„ç†æ•°æ®é›†ä¸ºArrowæ ¼å¼')
    parser.add_argument('--input', '-i', required=True, 
                       help='è¾“å…¥æ•°æ®é›†è·¯å¾„ (æ”¯æŒæ–‡ä»¶æˆ–ç›®å½•ï¼Œè‡ªåŠ¨æŸ¥æ‰¾ .json å’Œ .jsonl æ–‡ä»¶)')
    parser.add_argument('--output', '-o', required=True,
                       help='è¾“å‡ºæ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--force', '-f', action='store_true',
                       help='å¼ºåˆ¶è¦†ç›–è¾“å‡ºç›®å½•ï¼ˆä¸è¯¢é—®ï¼‰')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œé¢„å¤„ç†
    success = preprocess_gemini_dataset(args.input, args.output)
    
    if success:
        print("\nğŸ‰ æ•°æ®é¢„å¤„ç†æˆåŠŸå®Œæˆï¼")
        print(f"ç°åœ¨å¯ä»¥åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨: --dataset {args.output}")
    else:
        print("\nğŸ’¥ æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼")
        exit(1)

if __name__ == "__main__":
    main()