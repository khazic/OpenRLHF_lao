import argparse
import json
import math
import os
from datetime import timedelta

import torch
from torch import distributed as dist
from tqdm import tqdm

from openrlhf.datasets import SFTDataset
from openrlhf.datasets.utils import blending_datasets
from openrlhf.models import Actor, SFTLoss
from openrlhf.utils import get_strategy, get_tokenizer


def calculate_perplexity_per_sample(args):
    """
    è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å›°æƒ‘åº¦(Perplexity)ï¼Œç”¨äºæ•°æ®ç­›é€‰
    """
    # é…ç½®ç­–ç•¥
    strategy = get_strategy(args)
    strategy.setup_distributed(timeout=timedelta(minutes=180))

    # åŠ è½½æ¨¡å‹
    model = Actor(
        args.pretrain,
        attn_implementation=args.attn_implementation,
        bf16=args.bf16,
        load_in_4bit=args.load_in_4bit,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        target_modules=args.target_modules,
        lora_dropout=args.lora_dropout,
    )

    # é…ç½®tokenizer
    tokenizer = get_tokenizer(
        args.pretrain, model.model, "right", strategy, use_fast=not args.disable_fast_tokenizer
    )

    # å‡†å¤‡æ¨¡å‹
    model = strategy.prepare(model)
    model.eval()

    # åŠ è½½åŸå§‹æ•°æ®é›†ï¼ˆç”¨äºä¿å­˜æ—¶è·å–åŸå§‹å†…å®¹ï¼‰
    raw_dataset = blending_datasets(
        args.dataset,
        args.dataset_probs,
        strategy,
        args.seed,
        max_count=args.max_samples,
        dataset_split=args.dataset_split,
    )
    raw_dataset = raw_dataset.select(range(min(args.max_samples, len(raw_dataset))))
    
    # åŠ è½½SFTæ•°æ®é›†
    sft_dataset = SFTDataset(
        raw_dataset,
        tokenizer,
        args.max_len,
        strategy,
        pretrain_mode=args.pretrain_mode,
        input_template=args.input_template,
        multiturn=args.multiturn,
    )

    # å‡†å¤‡dataloader - batch_sizeè®¾ä¸º1ï¼Œè¿™æ ·å¯ä»¥å•ç‹¬è®¡ç®—æ¯ä¸ªæ ·æœ¬
    dataloader = strategy.setup_dataloader(
        sft_dataset,
        1,  # æ¯æ¬¡å¤„ç†ä¸€ä¸ªæ ·æœ¬
        True,
        False,
        sft_dataset.collate_fn,
        drop_last=False,
    )

    # åˆå§‹åŒ–losså‡½æ•°
    loss_fn = SFTLoss()

    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å›°æƒ‘åº¦
    dist.barrier()
    
    results = []
    sample_idx = 0

    pbar = tqdm(
        dataloader,
        desc="Calculating PPL per sample",
        disable=not strategy.is_rank_0(),
    )

    with torch.no_grad():
        for inputs, attention_masks, loss_masks in pbar:
            inputs = inputs.to(torch.cuda.current_device()).squeeze(1)
            attention_mask = attention_masks.to(torch.cuda.current_device()).squeeze(1)
            loss_mask = loss_masks.to(torch.cuda.current_device()).squeeze(1)

            # è·å–log probabilities
            per_token_log_probs = model(
                inputs,
                attention_mask=attention_mask,
                return_logprobs=True,
            )

            # è®¡ç®—è¯¥æ ·æœ¬çš„loss
            loss = loss_fn(per_token_log_probs, loss_mask[:, :-1])
            
            # è®¡ç®—è¯¥æ ·æœ¬çš„tokenæ•°ï¼ˆæœ‰æ•ˆtokenï¼Œä¸åŒ…æ‹¬paddingï¼‰
            num_tokens = loss_mask[:, :-1].sum().item()
            
            # è®¡ç®—PPL
            ppl = math.exp(loss.item())
            
            # è·å–åŸå§‹æ•°æ®
            raw_data = raw_dataset[sample_idx]
            
            # ä¿å­˜ç»“æœ
            result = {
                "index": sample_idx,
                "ppl": round(ppl, 4),
                "loss": round(loss.item(), 4),
                "num_tokens": int(num_tokens),
                args.input_key: raw_data[args.input_key],
                args.output_key: raw_data[args.output_key],
            }
            results.append(result)
            
            sample_idx += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                "current_ppl": f"{ppl:.2f}",
                "avg_ppl": f"{sum(r['ppl'] for r in results) / len(results):.2f}"
            })

    # æ”¶é›†æ‰€æœ‰GPUçš„ç»“æœ
    if strategy.is_rank_0():
        # æŒ‰PPLæ’åº
        results_sorted_by_ppl = sorted(results, key=lambda x: x['ppl'])
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        ppls = [r['ppl'] for r in results]
        avg_ppl = sum(ppls) / len(ppls)
        min_ppl = min(ppls)
        max_ppl = max(ppls)
        median_ppl = sorted(ppls)[len(ppls) // 2]
        
        print("\n" + "="*70)
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"æ€»æ ·æœ¬æ•°: {len(results)}")
        print(f"å¹³å‡PPL: {avg_ppl:.2f}")
        print(f"ä¸­ä½æ•°PPL: {median_ppl:.2f}")
        print(f"æœ€å°PPL: {min_ppl:.2f}")
        print(f"æœ€å¤§PPL: {max_ppl:.2f}")
        print("="*70)
        
        # å¦‚æœè®¾ç½®äº†é˜ˆå€¼ï¼Œæ˜¾ç¤ºç­›é€‰ä¿¡æ¯
        if args.ppl_threshold:
            kept_samples = [r for r in results if r['ppl'] <= args.ppl_threshold]
            removed_samples = [r for r in results if r['ppl'] > args.ppl_threshold]
            
            print(f"\nğŸ“Š ä½¿ç”¨é˜ˆå€¼ PPL <= {args.ppl_threshold} ç­›é€‰:")
            print(f"  âœ… ä¿ç•™æ ·æœ¬: {len(kept_samples)} ({len(kept_samples)/len(results)*100:.1f}%)")
            print(f"  âŒ åˆ é™¤æ ·æœ¬: {len(removed_samples)} ({len(removed_samples)/len(results)*100:.1f}%)")
            print("="*70)
        
        # ä¿å­˜æ‰€æœ‰æ ·æœ¬çš„PPLç»“æœ
        output_all = args.output_file.replace('.json', '_all.json')
        with open(output_all, 'w', encoding='utf-8') as f:
            json.dump(results_sorted_by_ppl, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æ‰€æœ‰æ ·æœ¬çš„PPLå·²ä¿å­˜åˆ°: {output_all}")
        
        # å¦‚æœè®¾ç½®äº†é˜ˆå€¼ï¼Œä¿å­˜ç­›é€‰åçš„æ•°æ®
        if args.ppl_threshold:
            # ä¿å­˜é«˜è´¨é‡æ•°æ®ï¼ˆä½PPLï¼‰
            output_kept = args.output_file.replace('.json', '_filtered.json')
            kept_data = [
                {args.input_key: r[args.input_key], args.output_key: r[args.output_key]} 
                for r in results if r['ppl'] <= args.ppl_threshold
            ]
            with open(output_kept, 'w', encoding='utf-8') as f:
                json.dump(kept_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ç­›é€‰åçš„é«˜è´¨é‡æ•°æ®å·²ä¿å­˜åˆ°: {output_kept}")
            
            # ä¿å­˜è¢«åˆ é™¤çš„æ•°æ®ï¼ˆé«˜PPLï¼‰
            output_removed = args.output_file.replace('.json', '_removed.json')
            removed_data = [
                {args.input_key: r[args.input_key], args.output_key: r[args.output_key], "ppl": r['ppl']} 
                for r in results if r['ppl'] > args.ppl_threshold
            ]
            with open(output_removed, 'w', encoding='utf-8') as f:
                json.dump(removed_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è¢«åˆ é™¤çš„ä½è´¨é‡æ•°æ®å·²ä¿å­˜åˆ°: {output_removed}")
        
        # æ˜¾ç¤ºPPLæœ€é«˜å’Œæœ€ä½çš„æ ·æœ¬
        print(f"\nğŸ” PPLæœ€ä½çš„3ä¸ªæ ·æœ¬ï¼ˆé«˜è´¨é‡ï¼‰:")
        for i, r in enumerate(results_sorted_by_ppl[:3], 1):
            question_preview = r[args.input_key][:100] + "..." if len(r[args.input_key]) > 100 else r[args.input_key]
            print(f"  {i}. PPL={r['ppl']:.2f}, Tokens={r['num_tokens']}")
            print(f"     {question_preview}\n")
        
        print(f"ğŸ”» PPLæœ€é«˜çš„3ä¸ªæ ·æœ¬ï¼ˆä½è´¨é‡ï¼‰:")
        for i, r in enumerate(results_sorted_by_ppl[-3:], 1):
            question_preview = r[args.input_key][:100] + "..." if len(r[args.input_key]) > 100 else r[args.input_key]
            print(f"  {i}. PPL={r['ppl']:.2f}, Tokens={r['num_tokens']}")
            print(f"     {question_preview}\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„PPLï¼Œç”¨äºæ•°æ®ç­›é€‰")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--pretrain", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--bf16", action="store_true", default=False, help="ä½¿ç”¨bfloat16")
    parser.add_argument(
        "--attn_implementation",
        type=str,
        default="flash_attention_2",
        help="æ³¨æ„åŠ›å®ç°æ–¹å¼",
    )
    parser.add_argument("--disable_fast_tokenizer", action="store_true", default=False)
    
    # LoRAå‚æ•°
    parser.add_argument("--load_in_4bit", action="store_true", default=False)
    parser.add_argument("--lora_rank", type=int, default=0)
    parser.add_argument("--lora_alpha", type=int, default=16)
    parser.add_argument("--target_modules", type=str, nargs="*", default="all-linear")
    parser.add_argument("--lora_dropout", type=float, default=0)
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--dataset", type=str, required=True, help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--dataset_probs", type=str, default=None)
    parser.add_argument("--dataset_split", type=str, default="train")
    parser.add_argument("--max_samples", type=int, default=1000000)
    parser.add_argument("--max_len", type=int, default=2048)
    parser.add_argument("--input_key", type=str, default="input", help="è¾“å…¥å­—æ®µåç§°")
    parser.add_argument("--output_key", type=str, default="output", help="è¾“å‡ºå­—æ®µåç§°")
    parser.add_argument("--input_template", type=str, default=None, help="è¾“å…¥æ¨¡æ¿")
    parser.add_argument("--pretrain_mode", action="store_true", default=False)
    parser.add_argument("--multiturn", action="store_true", default=False)
    parser.add_argument("--apply_chat_template", action="store_true", default=False)
    
    # ç­›é€‰å‚æ•°
    parser.add_argument(
        "--ppl_threshold", 
        type=float, 
        default=None, 
        help="PPLé˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„æ ·æœ¬å°†è¢«æ ‡è®°ä¸ºä½è´¨é‡ï¼ˆä¾‹å¦‚: 10.0ï¼‰"
    )
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--micro_batch_size", type=int, default=1)  # å›ºå®šä¸º1
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--zero_stage", type=int, default=0)
    parser.add_argument("--local_rank", type=int, default=-1)
    parser.add_argument("--full_determinism", action="store_true", default=False)
    parser.add_argument("--output_file", type=str, default="ppl_per_sample.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    calculate_perplexity_per_sample(args)

